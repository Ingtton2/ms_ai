import streamlit as st
import os
from openai import AzureOpenAI
from azure.storage.blob import BlobServiceClient
import PyPDF2
from io import BytesIO
import re
from dotenv import load_dotenv

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="AntBot - íŒ€ ì†”ë£¨ì…˜ ë„ìš°ë¯¸",
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS for cleaner UI
st.markdown("""
    <style>
    .main {
        padding: 0rem 1rem;
    }
    .stChatMessage {
        padding: 1rem;
        border-radius: 0.5rem;
    }
    .title-container {
        text-align: center;
        padding: 1rem 0 2rem 0;
    }
    .subtitle {
        color: #666;
        font-size: 1.1rem;
        margin-top: 0.5rem;
    }
    </style>
    """, unsafe_allow_html=True)

# Azure ì„¤ì • ì´ˆê¸°í™”
@st.cache_resource
def init_azure_clients():
    """Azure OpenAIì™€ Blob Storage í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
    
    # Azure OpenAI ì„¤ì •
    azure_endpoint = os.getenv("AZURE_OPENAI_ENDPOINT")
    azure_api_key = os.getenv("AZURE_OPENAI_API_KEY")
    azure_deployment = os.getenv("AZURE_OPENAI_DEPLOYMENT", "gpt-4")
    azure_api_version = os.getenv("AZURE_OPENAI_API_VERSION", "2024-02-15-preview")
    
    # Blob Storage ì„¤ì •
    blob_connection_string = os.getenv("AZURE_BLOB_CONNECTION_STRING")
    blob_container_name = os.getenv("AZURE_BLOB_CONTAINER_NAME", "antbot-docs")
    
    # í´ë¼ì´ì–¸íŠ¸ ìƒì„±
    openai_client = AzureOpenAI(
        azure_endpoint=azure_endpoint,
        api_key=azure_api_key,
        api_version=azure_api_version
    )
    
    blob_service_client = BlobServiceClient.from_connection_string(blob_connection_string)
    
    return openai_client, blob_service_client, azure_deployment, blob_container_name

@st.cache_data
def load_pdf_from_blob(_blob_service_client, container_name, _status=None, blob_name="AntBot_Manual.pdf"):
    """Blob Storageì—ì„œ PDF ë‹¤ìš´ë¡œë“œ ë° í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    try:
        if _status:
            _status.update(label="ğŸ“¦ Blobì—ì„œ PDF ë‹¤ìš´ë¡œë“œ ì¤‘...", state="running")
        
        # Blob í´ë¼ì´ì–¸íŠ¸ ìƒì„±
        blob_client = _blob_service_client.get_blob_client(
            container=container_name, 
            blob=blob_name
        )
        
        # PDF ë‹¤ìš´ë¡œë“œ
        download_stream = blob_client.download_blob()
        pdf_data = download_stream.readall()
        if _status:
            _status.update(label=f"âœ… PDF ë‹¤ìš´ë¡œë“œ ì™„ë£Œ ({len(pdf_data)/1024/1024:.1f} MB)", state="running")
        
        # PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
        if _status:
            _status.update(label="ğŸ“„ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)", state="running")
        pdf_file = BytesIO(pdf_data)
        pdf_reader = PyPDF2.PdfReader(pdf_file)
        
        text_content = ""
        total_pages = len(pdf_reader.pages)
        for i, page in enumerate(pdf_reader.pages):
            text_content += page.extract_text() + "\n"
            if _status and (i + 1) % 10 == 0:
                _status.update(label=f"ğŸ“„ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘... {i+1}/{total_pages} í˜ì´ì§€", state="running")
        
        if _status:
            _status.update(label=f"âœ… í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ ({total_pages}í˜ì´ì§€)", state="running")
        
        # í…ìŠ¤íŠ¸ ì²­í‚¹ (ê°„ë‹¨í•œ ë°©ë²•)
        if _status:
            _status.update(label="âœ‚ï¸ í…ìŠ¤íŠ¸ ì²­í‚¹ ì¤‘...", state="running")
        chunks = chunk_text(text_content)
        if _status:
            _status.update(label=f"âœ… ì²­í‚¹ ì™„ë£Œ ({len(chunks)}ê°œ ì²­í¬)", state="complete")
        
        return chunks, text_content
    except Exception as e:
        if _status:
            _status.update(label=f"âŒ ì˜¤ë¥˜ ë°œìƒ", state="error")
        error_msg = str(e)
        print(f"âŒ PDF ë¡œë”© ì˜¤ë¥˜: {error_msg}")  # í„°ë¯¸ë„ì— ì¶œë ¥
        import traceback
        print(traceback.format_exc())  # í„°ë¯¸ë„ì— ì „ì²´ ìŠ¤íƒ ì¶”ì  ì¶œë ¥
        st.error(f"âš ï¸ PDF ë¡œë”© ì¤‘ ì˜¤ë¥˜: {error_msg}")
        st.error(f"ğŸ“„ í™•ì¸: {blob_name} íŒŒì¼ì´ {container_name} ì»¨í…Œì´ë„ˆì— ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
        return [], ""

def chunk_text(text, chunk_size=1000, overlap=200):
    """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• """
    # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• 
    sentences = re.split(r'(?<=[.!?])\s+', text)
    
    chunks = []
    current_chunk = ""
    
    for sentence in sentences:
        if len(current_chunk) + len(sentence) < chunk_size:
            current_chunk += sentence + " "
        else:
            if current_chunk:
                chunks.append(current_chunk.strip())
            current_chunk = sentence + " "
    
    if current_chunk:
        chunks.append(current_chunk.strip())
    
    return chunks

def find_relevant_context(query, chunks, top_k=3):
    """ì¿¼ë¦¬ì™€ ê´€ë ¨ëœ ì»¨í…ìŠ¤íŠ¸ ì°¾ê¸° (ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­)"""
    # ë” ì •êµí•œ ë°©ë²•: Azure OpenAI Embeddings ì‚¬ìš©
    # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­ ì‚¬ìš©
    query_keywords = set(query.lower().split())
    
    chunk_scores = []
    for chunk in chunks:
        chunk_keywords = set(chunk.lower().split())
        score = len(query_keywords.intersection(chunk_keywords))
        chunk_scores.append((score, chunk))
    
    # ì ìˆ˜ ê¸°ì¤€ ì •ë ¬ ë° ìƒìœ„ kê°œ ì„ íƒ
    chunk_scores.sort(reverse=True, key=lambda x: x[0])
    relevant_chunks = [chunk for score, chunk in chunk_scores[:top_k] if score > 0]
    
    return relevant_chunks

def get_chatbot_response(openai_client, deployment, query, context_chunks):
    """Azure OpenAIë¥¼ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µ ìƒì„±"""
    
    # ì»¨í…ìŠ¤íŠ¸ ê²°í•©
    context = "\n\n".join(context_chunks) if context_chunks else "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
    system_message = """ë‹¹ì‹ ì€ AntBotìœ¼ë¡œ, ìš°ë¦¬ íŒ€ì˜ ì†”ë£¨ì…˜ ë§¤ë‰´ì–¼ì„ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ì¹œì ˆí•œ ë„ìš°ë¯¸ì…ë‹ˆë‹¤.
    
ë‹¤ìŒ ê·œì¹™ì„ ë”°ë¼ì£¼ì„¸ìš”:
1. ì œê³µëœ ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
2. ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ê³ , ëª¨ë¥¸ë‹¤ê³  ì†”ì§íˆ ë§í•˜ì„¸ìš”.
3. ë‹µë³€ì€ ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ì‘ì„±í•˜ì„¸ìš”.
4. í•„ìš”í•œ ê²½ìš° ë‹¨ê³„ë³„ë¡œ ì„¤ëª…í•˜ì„¸ìš”.
5. í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”."""

    user_message = f"""ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸:
{context}

ì‚¬ìš©ì ì§ˆë¬¸: {query}

ìœ„ ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ì ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”."""

    try:
        response = openai_client.chat.completions.create(
            model=deployment,
            messages=[
                {"role": "system", "content": system_message},
                {"role": "user", "content": user_message}
            ],
            temperature=0.7,
            max_tokens=1000
        )
        
        return response.choices[0].message.content
    except Exception as e:
        return f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

# ë©”ì¸ UI
def main():
    # í—¤ë”
    st.markdown("""
        <div class="title-container">
            <h1>ğŸ¤– AntBot</h1>
            <p class="subtitle">íŒ€ ì†”ë£¨ì…˜ ë§¤ë‰´ì–¼ ê¸°ë°˜ AI ë„ìš°ë¯¸</p>
        </div>
    """, unsafe_allow_html=True)
    
    # ì‚¬ì´ë“œë°”
    with st.sidebar:
        st.header("âš™ï¸ ì„¤ì •")
        
        # í™˜ê²½ ë³€ìˆ˜ í™•ì¸
        env_check = {
            "Azure OpenAI Endpoint": bool(os.getenv("AZURE_OPENAI_ENDPOINT")),
            "Azure OpenAI API Key": bool(os.getenv("AZURE_OPENAI_API_KEY")),
            "Blob Connection String": bool(os.getenv("AZURE_BLOB_CONNECTION_STRING"))
        }
        
        st.subheader("ì—°ê²° ìƒíƒœ")
        for key, value in env_check.items():
            status = "âœ…" if value else "âŒ"
            st.text(f"{status} {key}")
        
        st.divider()
        
        # ë””ë²„ê·¸ ì •ë³´
        if st.checkbox("ğŸ” ë””ë²„ê·¸ ì •ë³´ í‘œì‹œ", value=False):
            st.subheader("ë””ë²„ê·¸ ì •ë³´")
            st.code(f"Endpoint: {os.getenv('AZURE_OPENAI_ENDPOINT')}")
            st.code(f"Deployment: {os.getenv('AZURE_OPENAI_DEPLOYMENT', 'gpt-4')}")
            st.code(f"Container: {os.getenv('AZURE_BLOB_CONTAINER_NAME', 'antbot-docs')}")
        
        st.divider()
        
        st.subheader("ğŸ“š ë¬¸ì„œ ì •ë³´")
        st.info("**ë§¤ë‰´ì–¼:** antbot_manual.pdf")
        
        st.divider()
        
        if st.button("ğŸ—‘ï¸ ëŒ€í™” ì´ˆê¸°í™”", use_container_width=True):
            st.session_state.messages = []
            st.rerun()
        
        st.divider()
        st.caption("Made with â¤ï¸ for the team")
    
    # Azure í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    try:
        openai_client, blob_service_client, deployment, container_name = init_azure_clients()
        
        # PDF ë¬¸ì„œ ë¡œë“œ
        status_placeholder = st.empty()
        with status_placeholder.status("ğŸ“„ ë§¤ë‰´ì–¼ ë¡œë”© ì¤‘... ëŒ€ìš©ëŸ‰ íŒŒì¼(294MB)ì´ë¯€ë¡œ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.", expanded=True) as status:
            chunks, full_text = load_pdf_from_blob(blob_service_client, container_name, _status=status)
        status_placeholder.empty()
        
        if not chunks:
            st.error("âš ï¸ ë§¤ë‰´ì–¼ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Blob Storage ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
            st.stop()
        
        # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
        if "messages" not in st.session_state:
            st.session_state.messages = []
        
        # ì±„íŒ… ê¸°ë¡ í‘œì‹œ
        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])
        
        # ì‚¬ìš©ì ì…ë ¥
        if prompt := st.chat_input("ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"):
            # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
            st.session_state.messages.append({"role": "user", "content": prompt})
            with st.chat_message("user"):
                st.markdown(prompt)
            
            # ë´‡ ì‘ë‹µ ìƒì„±
            with st.chat_message("assistant"):
                with st.spinner("ìƒê° ì¤‘..."):
                    # ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ ì°¾ê¸°
                    relevant_chunks = find_relevant_context(prompt, chunks)
                    
                    # ì‘ë‹µ ìƒì„±
                    response = get_chatbot_response(
                        openai_client, 
                        deployment, 
                        prompt, 
                        relevant_chunks
                    )
                    
                    st.markdown(response)
            
            # ë´‡ ë©”ì‹œì§€ ì¶”ê°€
            st.session_state.messages.append({"role": "assistant", "content": response})
        
        # ì²« ë°©ë¬¸ ì‹œ í™˜ì˜ ë©”ì‹œì§€
        if len(st.session_state.messages) == 0:
            st.info("ğŸ‘‹ ì•ˆë…•í•˜ì„¸ìš”! AntBotì…ë‹ˆë‹¤. íŒ€ ì†”ë£¨ì…˜ ë§¤ë‰´ì–¼ì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì„ ë¬¼ì–´ë³´ì„¸ìš”!")
            
    except Exception as e:
        st.error(f"âš ï¸ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        st.info("ğŸ’¡ .env íŒŒì¼ ë˜ëŠ” í™˜ê²½ ë³€ìˆ˜ê°€ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")

if __name__ == "__main__":
    main()
